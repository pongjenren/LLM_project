# How to run
## Environment
Just install all modules in ```requirements.txt```
> notice ```llama-cpp-python``` need special insturction!
## Add pdf
1. Add pdf into data/papers
2. run ```build_db.py```
## Run GUI
1. run ```main.py```

# About backbone
**LLM: Llama 3 8B Instruct Q4 K M GGUF**

I'm using GPU with only 8GB VRAM, if your VRAM is >= 16GB, consider using native Llama 3 8B by inversing ```Change to GGUF``` steps.

**Embedding: e5-large-v2**

# Chagne to GGUF:
1. install llama-cpp-python ```CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python```
1. download model: https://huggingface.co/NoelJacob/Meta-Llama-3-8B-Instruct-Q4_K_M-GGUF/tree/main
2. revise: src/config.py
3. add: src/llm_gguf.py
4. revise: src/rag_pipeline.py



# ğŸ“š Personal Research Notes Assistant

*A local RAG system for reading & querying personal PDF research papers using Llama 3 (GGUF) + ChromaDB + E5 embeddings*

---

## âœ¨ Overview

This project implements a fully local **Retrieval-Augmented Generation (RAG)** system that turns your personal research papers (PDF) into a semantic knowledge base you can query in natural language.

åŠŸèƒ½åŒ…å«ï¼š

* ğŸ” **PDF ingestion**ï¼šè®€å–è«–æ–‡ä¸¦åˆ‡æˆèªæ„ç‰‡æ®µ
* ğŸ§  **Embedding + ChromaDB**ï¼šä»¥ E5-large-v2 å»ºç«‹å‘é‡è³‡æ–™åº«
* ğŸ¯ **Hybrid Retrieval**ï¼ˆå¯åŠ å…¥ BM25ï¼‰âš ï¸ Not implemented yet âš ï¸
* ğŸ¤– **LLM generation using Llama 3 8B GGUF**ï¼ˆllama-cpp-pythonï¼‰
* ğŸ“ **Long-term memory module**ï¼ˆè‡ªå‹•æ‘˜è¦æœ€è¿‘å°è©±ï¼‰âš ï¸ Not implemented yet âš ï¸
* ğŸ’¬ **Gradio UI**ï¼ˆäº’å‹•å¼æŸ¥è©¢ï¼‰

å®Œå…¨åœ¨æœ¬åœ°åŸ·è¡Œï¼Œ**ä¸éœ€è¦ä»»ä½• API é‡‘é‘°ã€ä¸ä¸Šå‚³è³‡æ–™ã€ä¸ä¾è³´å¤–éƒ¨é›²ç«¯**ã€‚

---

## ğŸ›  System Architecture

```
PDFs â†’ Preprocess â†’ Chunks â†’ E5 Embeddings â†’ ChromaDB  
                                  â†“
                       Hybrid Retrieval (semantic/keyword)
                                  â†“
                     Memory Module (session summary)
                                  â†“
                 Llama 3 8B (GGUF, llama-cpp-python)
                                  â†“
                      Final Answer Generation
```

---

## ğŸ“ Project Structure

```
LLM_project/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ build_db.py
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ llama-3-8b-instruct-q4_k_m.gguf
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ papers/          # æ”¾ä½ çš„ PDF
â”‚   â”œâ”€â”€ processed/       # ä¸­é–“è³‡æ–™
â”‚   â”œâ”€â”€ vectordb/        # ChromaDB å„²å­˜ä½ç½®
â”‚   â””â”€â”€ memory.txt       # é•·æœŸè¨˜æ†¶
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ embed_store.py
â”‚   â”œâ”€â”€ retrieve.py
â”‚   â”œâ”€â”€ memory.py
â”‚   â”œâ”€â”€ llm_gguf.py
â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â”œâ”€â”€ baseline_keyword.py
â”‚   â”œâ”€â”€ baseline_vanilla_rag.py
â”‚   â””â”€â”€ evaluation.py
â”‚
â””â”€â”€ app/
    â””â”€â”€ ui_gradio.py
```

---

## ğŸ’¾ Installation

### 1. å»ºç«‹ conda / venvï¼ˆæ¨è–¦ condaï¼‰

```bash
conda create -n LLM_env python=3.10 -y
conda activate LLM_env
```

### 2. å®‰è£ä¾è³´

> **å¼·çƒˆå»ºè­°ä½¿ç”¨é ç·¨è­¯ CUDA ç‰ˆæœ¬ â†’ å®‰è£æˆåŠŸç‡ 100%**

```bash
pip install -r requirements.txt
pip install llama-cpp-python-cu122
```

> å¦‚æœä½ ä½¿ç”¨ CPUï¼š
> `pip install llama-cpp-python`

---

## ğŸ”½ Download GGUF Model

ä½¿ç”¨ä½ å–œæ­¡çš„ Llama 3 GGUF é‡åŒ–æ¨¡å‹ï¼Œä¾‹å¦‚ï¼š

* `Llama-3-8B-Instruct-Q4_K_M.gguf`
* `Llama-3-8B-Instruct-Q5_K_M.gguf`

å¾ HuggingFace ä¸‹è¼‰ï¼ˆä¾‹å¦‚ TheBloke æˆ– bartowskiï¼‰ã€‚

æ”¾åˆ°ï¼š

```
models/llama-3-8b-instruct-q4_k_m.gguf
```

---

## ğŸ“„ Adding PDF Papers

æŠŠä»»ä½• `.pdf` æ”¾é€²ï¼š

```
data/papers/
```

ä¾‹ï¼š

```
data/papers/
    diffusion_models_paper.pdf
    llm_agents_survey.pdf
```

---

## ğŸ— Build Vector Database

è·‘ï¼š

```bash
python build_db.py
```

æœƒè¼¸å‡ºï¼š

```
ğŸ“„ Loading and preprocessing PDFs...
âœ” Loaded XXX chunks
ğŸ§  Initializing vector store...
ğŸ“¥ Adding into ChromaDB...
ğŸ‰ All PDFs have been added into the RAG database!
```

---

## ğŸš€ Run the UI

```bash
python main.py
```

é–‹å•Ÿä½ çš„ç€è¦½å™¨ï¼ŒGradio ä»‹é¢æœƒè®“ä½ ï¼š

* è¼¸å…¥è‡ªç„¶èªè¨€å•é¡Œ
* æª¢ç´¢ä½ çš„ PDF ä¸­çš„æ®µè½
* ç”± Llama 3 GGUF ç”Ÿæˆç­”æ¡ˆ
* å¼•ç”¨ chunk èˆ‡ paper title

---

## ğŸ”§ Configuration

æ‰€æœ‰é…ç½®æ”¾åœ¨ï¼š

```
src/config.py
```

ä½ å¯ä»¥èª¿æ•´ï¼š

* Embedding modelï¼ˆE5-large-v2ï¼‰
* GGUF model è·¯å¾‘
* context windowï¼ˆ8k/16kï¼‰
* retrieval Top-K
* memory file path

å¦‚æœä½ åªæœ‰ 8GB VRAMï¼Œå»ºè­°ï¼š

```python
N_GPU_LAYERS = -1     # è‡ªå‹•æ”¾åˆ° GPU
N_CTX = 8192
```



## ğŸ™Œ Acknowledgements

This project uses:

* **Meta Llama 3**
* **llama.cpp / llama-cpp-python**
* **ChromaDB**
* **SentenceTransformers (E5-large-v2)**
* **Gradio**
* **LangChain text splitters**